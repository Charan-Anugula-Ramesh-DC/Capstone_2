{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('..\\TrialFAQHandling\\BankFAQs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do I need to enter ‘#’ after keying in my Card...</td>\n",
       "      <td>Please listen to the recorded message and foll...</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What details are required when I want to perfo...</td>\n",
       "      <td>To perform a secure IVR transaction, you will ...</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How should I get the IVR Password  if I hold a...</td>\n",
       "      <td>An IVR password can be requested only from the...</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I register my Mobile number for IVR Pas...</td>\n",
       "      <td>Please call our Customer Service Centre and en...</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I obtain an IVR Password</td>\n",
       "      <td>By Sending SMS request: Send an SMS 'PWD&lt;space...</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Do I need to enter ‘#’ after keying in my Card...   \n",
       "1  What details are required when I want to perfo...   \n",
       "2  How should I get the IVR Password  if I hold a...   \n",
       "3  How do I register my Mobile number for IVR Pas...   \n",
       "4                  How can I obtain an IVR Password    \n",
       "\n",
       "                                              Answer     Class  \n",
       "0  Please listen to the recorded message and foll...  security  \n",
       "1  To perform a secure IVR transaction, you will ...  security  \n",
       "2  An IVR password can be requested only from the...  security  \n",
       "3  Please call our Customer Service Centre and en...  security  \n",
       "4  By Sending SMS request: Send an SMS 'PWD<space...  security  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['Class'], inplace =  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Question'] = df['Question'].apply(lambda x : 'Question : ' + x if isinstance(x, str) else x)\n",
    "df['Answer'] = df['Answer'].apply(lambda x : 'Answer : ' + x if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('..\\TrialFAQHandling\\FAQ_RAG.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "faq_data = pd.read_csv(\"BankFAQs.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "faq_data['Question'] = faq_data['Question'].str.lower().str.replace('[^a-zA-Z\\s]', '')\n",
    "faq_data['Answer'] = faq_data['Answer'].str.lower().str.replace('[^a-zA-Z\\s]', '')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(faq_data['Question'], faq_data['Class'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(faq_data['Class'].unique()))\n",
    "\n",
    "# Encode questions and classes\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "for question, class_label in zip(faq_data['Question'], faq_data['Class']):\n",
    "    encoded_dict = tokenizer.encode_plus(question, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    labels.append(class_label)\n",
    "\n",
    "# Convert labels to PyTorch tensor\n",
    "label_map = {label: idx for idx, label in enumerate(faq_data['Class'].unique())}\n",
    "labels = [label_map[label] for label in labels]\n",
    "labels = torch.LongTensor(labels)\n",
    "\n",
    "# Fine-tune the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    for input_id, attention_mask, label in zip(input_ids, attention_masks, labels):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_id, attention_mask=attention_mask, labels=label)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Predict answer for a user query\n",
    "# ... (same as before)\n",
    "        \n",
    "\n",
    "# Predict answer for a user query\n",
    "def get_answer(user_query):\n",
    "    input_dict = tokenizer.encode_plus(user_query, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "    outputs = model(input_dict['input_ids'], input_dict['attention_mask'])\n",
    "    predicted_class = outputs.logits.argmax(-1).item()\n",
    "    relevant_data = faq_data[faq_data['Class'] == predicted_class]\n",
    "    # ... (find the most similar question and retrieve the answer)\n",
    "    # relevant_data = faq_data #[faq_data['Class'] == predicted_class]\n",
    "    if not relevant_data.empty:\n",
    "        questions_vec = vectorizer.transform(relevant_data['Question'])\n",
    "        similarities = cosine_similarity(user_query_vec, questions_vec)\n",
    "        most_similar_index = similarities.argmax()\n",
    "        most_similar_question = relevant_data.iloc[most_similar_index]\n",
    "        answer = most_similar_question['Answer']\n",
    "        return answer\n",
    "    else:\n",
    "        return \"Sorry, I couldn't find a relevant answer in the FAQ.\"\n",
    "    \n",
    "# Example usage\n",
    "user_query = \"How long your OTP is valid?\" #What is the validity period of the OTP\"\n",
    "answer = get_answer(user_query)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['audio-classification', 'automatic-speech-recognition', 'conversational', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection']\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines import PIPELINE_REGISTRY\n",
    "\n",
    "#Get the list of tasks that are supported by Huggingface pipeline\n",
    "print(PIPELINE_REGISTRY.get_supported_tasks())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default Model for Sentiment Analysis: \n",
      "{'model': {'pt': ('distilbert/distilbert-base-uncased-finetuned-sst-2-english', 'af0f99b'), 'tf': ('distilbert/distilbert-base-uncased-finetuned-sst-2-english', 'af0f99b')}}\n"
     ]
    }
   ],
   "source": [
    "#Get information about a specific task\n",
    "print(\"\\nDefault Model for Sentiment Analysis: \")\n",
    "print(PIPELINE_REGISTRY.check_task('sentiment-analysis')[1].get('default'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huggingface Cache directory is :  C:\\Users\\nanth/.cache/huggingface/hub\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.locks',\n",
       " 'models--cardiffnlp--twitter-roberta-base-sentiment-latest',\n",
       " 'models--deepset--roberta-base-squad2',\n",
       " 'models--distilbert--distilbert-base-uncased-finetuned-sst-2-english',\n",
       " 'models--distilbert-base-cased-distilled-squad',\n",
       " 'models--distilbert-base-uncased',\n",
       " 'models--facebook--s2t-small-librispeech-asr',\n",
       " 'models--facebook--wav2vec2-base-960h',\n",
       " 'models--openai--whisper-large-v3',\n",
       " 'models--openai--whisper-tiny.en',\n",
       " 'models--SeamlessM4T',\n",
       " 'models--speechbrain--sepformer-wham',\n",
       " 'models--speechbrain--sepformer-whamr',\n",
       " 'version.txt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "\n",
    "#Load a pipeline. This will download the model checkpoint from huggingface and cache it\n",
    "#locally on disk. If model is already available in cache, it will simply use the cached version\n",
    "#Download will usually take a long time, depending on network bandwidth\n",
    "\n",
    "sentiment_classifier = pipeline(task = \"sentiment-analysis\" , model = 'cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "\n",
    "#Cache usually available at : <<user-home>>.cache\\huggingface\\hub\n",
    "\n",
    "cache_dir = os.path.expanduser('~') + \"/.cache/huggingface/hub\"\n",
    "print(\"Huggingface Cache directory is : \", cache_dir)\n",
    "\n",
    "#Contents of cache directory\n",
    "os.listdir(cache_dir)\n",
    "\n",
    "#Predict sentiment using the pipeline\n",
    "sentiment_results=sentiment_classifier(\"This is a great course\")\n",
    "print(sentiment_results) #sentiment_results[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9754324555397034}]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 298k/298k [00:00<00:00, 4.06MB/s]\n",
      "Downloading data: 100%|██████████| 93.9k/93.9k [00:00<00:00, 1.33MB/s]\n",
      "Generating train split: 100%|██████████| 10003/10003 [00:00<00:00, 341883.68 examples/s]\n",
      "Generating test split: 100%|██████████| 3080/3080 [00:00<00:00, 385039.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"PolyAI/banking77\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"PolyAI/banking77\")\n",
    "\n",
    "# The dataset is a dictionary containing the train and test splits\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# You can access the data in each split as a list\n",
    "train_texts = train_dataset['text']\n",
    "train_labels = train_dataset['label']\n",
    "\n",
    "# You can also convert the dataset to a pandas DataFrame\n",
    "import pandas as pd\n",
    "train_df = pd.DataFrame({'text': train_texts, 'label': train_labels})\n",
    "\n",
    "# Now you can use the data for training a model, performing analysis, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
